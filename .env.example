# Enhanced Book LLM Twin Configuration Template
# Copy this file to .env and customize: cp .env.example .env

# ============================================
# DOCUMENT PROCESSING & STORAGE
# ============================================
DATA_DIR=data/raw

# ============================================
# QDRANT VECTOR DATABASE
# ============================================
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=book

# ============================================
# OLLAMA LLM CONFIGURATION
# ============================================
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_NUM_PREDICT=256
OLLAMA_TEMPERATURE=0.2

# ============================================
# EMBEDDING MODEL
# ============================================
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Alternative models (uncomment to try):
# EMBEDDING_MODEL=intfloat/e5-base-v2              # Better quality, 768d
# EMBEDDING_MODEL=BAAI/bge-small-en-v1.5          # Good multilingual support

# ============================================
# ENHANCED CHUNKING CONFIGURATION
# ============================================

# Basic Chunking Settings
CHUNK_TOKENS=250
OVERLAP_TOKENS=40                  # Legacy - kept for backward compatibility

# Advanced Chunking Settings
OVERLAP_SENTENCES=2                # Sentence-based overlap (more semantic than word-based)
CHUNKING_STRATEGY=hybrid           # Options: sentence_based, semantic_similarity, token_aware, hybrid

# Semantic Similarity Settings
SIMILARITY_THRESHOLD=0.5           # 0.3=more chunks, 0.7=fewer chunks (recommended: 0.5)

# ============================================
# ENHANCED RETRIEVAL CONFIGURATION
# ============================================

# Hybrid Search Settings
ENABLE_HYBRID_SEARCH=false         # Combine vector + keyword search
VECTOR_WEIGHT=0.7                  # Weight for vector search (0.0-1.0)
BM25_WEIGHT=0.3                    # Weight for BM25 search (0.0-1.0)

# BM25 Index Settings
BM25_INDEX_PATH=data/bm25_book     # Path to BM25 index files
BM25_K1=1.2                       # Term frequency normalization
BM25_B=0.75                       # Length normalization
BM25_EPSILON=0.25                 # Minimum score threshold

# Reranking Configuration
ENABLE_RERANKING=false             # Use cross-encoder reranking
RERANKING_MODEL=cross-encoder/ms-marco-MiniLM-L-2-v2
RERANK_TOP_K=20                    # Number of results to rerank
RERANK_BATCH_SIZE=32               # Batch size for reranking

# Query Processing
ENABLE_QUERY_EXPANSION=false       # Expand queries for better coverage
MAX_QUERY_EXPANSIONS=3             # Maximum expanded queries to generate
MIN_QUERY_LENGTH=3                 # Minimum query length
ENABLE_SYNONYM_EXPANSION=true      # Use synonym-based expansion
ENABLE_QUERY_REFORMULATION=true    # Reformulate queries
ENABLE_QUESTION_VARIANTS=true      # Generate question variants

# ============================================
# RAG PIPELINE SETTINGS  
# ============================================
PROMPT_MAX_CONTEXTS=4
PROMPT_MAX_CHARS_PER_CONTEXT=1200

# Enhanced Retrieval Settings
TOP_K=5                            # Final number of chunks to retrieve

# ============================================
# ADVANCED FEATURES
# ============================================

# Performance & Debug
ENABLE_CHUNKING_STATS=true         # Show detailed chunking statistics
LOG_LEVEL=INFO                     # DEBUG, INFO, WARNING, ERROR

# ============================================
# SETUP INSTRUCTIONS
# ============================================
# 1. Copy this file: cp .env.example .env
# 2. Install dependencies: pip install -r requirements.txt rank-bm25
# 3. Start services: docker compose up -d && ollama serve
# 4. Add documents to data/raw/ folder
# 5. Run ingestion: python scripts/ingest.py
# 6. Start UI: streamlit run app/streamlit_app.py

# ============================================
# CONFIGURATION GUIDE
# ============================================
# CHUNKING_STRATEGY options:
# - sentence_based:     Fast, simple, good for most documents
# - semantic_similarity: Groups related sentences, better context
# - token_aware:        Strict token limits, prevents oversized chunks  
# - hybrid:             Best of all worlds (RECOMMENDED)

# SIMILARITY_THRESHOLD guide:
# - 0.3-0.4: More chunks, very fine-grained splitting
# - 0.5-0.6: Balanced chunking (RECOMMENDED)  
# - 0.7-0.8: Fewer chunks, keeps more context together

# HYBRID SEARCH guide:
# - Start with ENABLE_HYBRID_SEARCH=false for testing
# - Enable after building BM25 index during ingestion
# - VECTOR_WEIGHT 0.7 + BM25_WEIGHT 0.3: Balanced (RECOMMENDED)
# - Higher vector weight: Better semantic understanding
# - Higher BM25 weight: Better keyword matching

# RERANKING guide:
# - Improves result quality by 10-20%
# - Requires additional model download (~100MB)
# - Slower but more accurate results
# - Use RERANK_TOP_K=20 for good speed/quality balance

# QUERY EXPANSION guide:
# - Helps with complex or ambiguous queries
# - Generates alternative query formulations
# - May increase processing time but improves coverage
# - Start with ENABLE_QUERY_EXPANSION=false for testing