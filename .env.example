# Enhanced Book LLM Twin Configuration Template
# Copy this file to .env and customize: cp .env.example .env

# ============================================
# DOCUMENT PROCESSING & STORAGE
# ============================================
DATA_DIR=data/raw

# ============================================
# QDRANT VECTOR DATABASE
# ============================================
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=
QDRANT_COLLECTION=book

# ============================================
# OLLAMA LLM CONFIGURATION
# ============================================
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_NUM_PREDICT=256
OLLAMA_TEMPERATURE=0.2

# ============================================
# EMBEDDING MODEL
# ============================================
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Alternative models (uncomment to try):
# EMBEDDING_MODEL=intfloat/e5-base-v2              # Better quality, 768d
# EMBEDDING_MODEL=BAAI/bge-small-en-v1.5          # Good multilingual support

# ============================================
# ENHANCED CHUNKING CONFIGURATION
# ============================================

# Basic Chunking Settings
CHUNK_TOKENS=250
OVERLAP_TOKENS=40                  # Legacy - kept for backward compatibility

# Advanced Chunking Settings
OVERLAP_SENTENCES=2                # Sentence-based overlap (more semantic than word-based)
CHUNKING_STRATEGY=hybrid           # Options: sentence_based, semantic_similarity, token_aware, hybrid

# Semantic Similarity Settings
SIMILARITY_THRESHOLD=0.5           # 0.3=more chunks, 0.7=fewer chunks (recommended: 0.5)

# ============================================
# RAG PIPELINE SETTINGS  
# ============================================
PROMPT_MAX_CONTEXTS=4
PROMPT_MAX_CHARS_PER_CONTEXT=1200

# Enhanced Retrieval Settings
TOP_K=5                            # Number of chunks to retrieve

# ============================================
# ADVANCED FEATURES (Optional)
# ============================================

# Performance & Debug
ENABLE_CHUNKING_STATS=true         # Show detailed chunking statistics
LOG_LEVEL=INFO                     # DEBUG, INFO, WARNING, ERROR

# Experimental Features (set to true to enable)
ENABLE_HYBRID_SEARCH=false         # Combine vector + keyword search
ENABLE_QUERY_EXPANSION=false       # Expand queries for better retrieval
ENABLE_RERANKING=false             # Rerank results for better relevance

# ============================================
# CONFIGURATION GUIDE
# ============================================
# CHUNKING_STRATEGY options:
# - sentence_based:     Fast, simple, good for most documents
# - semantic_similarity: Groups related sentences, better context
# - token_aware:        Strict token limits, prevents oversized chunks  
# - hybrid:             Best of all worlds (RECOMMENDED)

# SIMILARITY_THRESHOLD guide:
# - 0.3-0.4: More chunks, very fine-grained splitting
# - 0.5-0.6: Balanced chunking (RECOMMENDED)  
# - 0.7-0.8: Fewer chunks, keeps more context together

# ============================================
# SETUP INSTRUCTIONS
# ============================================
# 1. Copy this file: cp .env.example .env
# 2. Install dependencies: pip install -r requirements.txt
# 3. Start services: docker compose up -d && ollama serve
# 4. Add documents to data/raw/ folder
# 5. Run ingestion: python scripts/ingest.py
# 6. Start UI: streamlit run app/streamlit_app.py